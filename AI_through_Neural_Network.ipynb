{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_neural_network:\n",
    "    \n",
    "    def __init__(self, hidden_layer_plan):\n",
    "        \n",
    "        '''\n",
    "        Initialization function.\n",
    "        \n",
    "        input: \n",
    "        layer_plan : data_type = list\n",
    "        \n",
    "        List cointaing the number of neurons in every layers for the neural network. The first and last elements of\n",
    "        layer_plan corresponds to number of input features and the output value. \n",
    "        The rest of the elements between them corresponds to the neurons in hidden layers.\n",
    "        More the Hidden Layers implies deeper the network!\n",
    "        '''\n",
    "        \n",
    "        #dataset\n",
    "        self.dataset = None\n",
    "        \n",
    "        #input and output data\n",
    "        self.input_data = None\n",
    "        self.output_data = None\n",
    "        self.predicted_output =None\n",
    "        \n",
    "        #train and test data\n",
    "        self.training_data = None\n",
    "        self.testing_data = None\n",
    "        \n",
    "        #layer details\n",
    "        self.hidden_layer_plan = hidden_layer_plan\n",
    "        self.layer_dimensions = None\n",
    "        self.num_of_layers = None\n",
    "        \n",
    "        #Batching\n",
    "        self.mini_batches = []\n",
    "        \n",
    "        #Forward Activation\n",
    "        self.activations, self.Z = [], []\n",
    "        \n",
    "        #Backward Activation\n",
    "        self.dActivations = []\n",
    "        \n",
    "        #Error_method and loss\n",
    "        self.error_method = None\n",
    "        self.loss = None\n",
    "        self.cost = []\n",
    "        self.cost_value = None\n",
    "        \n",
    "        #Network parameters\n",
    "        self.network_parameters, self.gradient_network_parameters = {}, {}\n",
    "        self.steep_parameters, self.RMS_parameters = {}, {}\n",
    "        #optimizer\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        ''' Representative function to welcome the user for collaborating with the new AI trainer'''\n",
    "        return 'Hello there!! I am your new AI trainer......!'\n",
    "    \n",
    "    \n",
    "    def load_dataset(self, file):\n",
    "        ''' \n",
    "        To load dataset \n",
    "        \n",
    "        input:\n",
    "        file : data_type: string\n",
    "        File name consisting the inputs and outputs. The last column in the file is the output.\n",
    "        '''\n",
    "        self.dataset = np.loadtxt(file)\n",
    "        self.input_data = self.dataset[:,:-1]\n",
    "        self.output_data = self.dataset[:,-1]\n",
    "        \n",
    "        #Normalization\n",
    "        max_inputs = np.amax(self.input_data, axis=0)\n",
    "        min_inputs = np.amin(self.input_data, axis=0)\n",
    "        diff_inputs = max_inputs - min_inputs\n",
    "        self.input_data = np.divide((self.input_data - min_inputs),diff_inputs)\n",
    "        \n",
    "        max_outputs = np.amax(self.output_data, axis=0)\n",
    "        min_outputs = np.amin(self.output_data, axis=0)\n",
    "        diff_outputs = max_outputs - min_outputs\n",
    "        self.output_data = np.divide((self.output_data - min_outputs), diff_outputs)\n",
    "        \n",
    "        #Layer planning\n",
    "        self.layer_dimensions = [self.input_data.shape[1]]+self.hidden_layer_plan+[1] #1 corresponds to columns in O/P\n",
    "        self.num_of_layers = len(self.layer_dimensions)\n",
    "        \n",
    "    def test_train_split(self, split = 0.7):\n",
    "        '''\n",
    "        This function is utilized to segragate the complete data into training and testing data.\n",
    "        '''\n",
    "        \n",
    "        n_total = int(self.dataset.shape[0])\n",
    "        n_train = int(split*n_total)\n",
    "        \n",
    "        mask = np.zeros((n_total), dtype=bool)\n",
    "        mask[:n_train] = True\n",
    "        \n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        X_train = self.input_data[mask]\n",
    "        Y_train = self.output_data[mask]\n",
    "        \n",
    "        X_test = self.input_data[~mask]\n",
    "        Y_test = self.output_data[~mask]\n",
    "        \n",
    "        self.training_data = (X_train.transpose(), Y_train)\n",
    "        self.testing_data = (X_test.transpose(), Y_test)\n",
    "    \n",
    "    def network_parameters_initialization(self):\n",
    "        np.random.seed(1)\n",
    "        for i in range(1,self.num_of_layers):\n",
    "            self.network_parameters['Weights'+str(i)] = np.random.randn(self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1])\n",
    "            self.network_parameters['bias'+str(i)] = np.random.randn(self.layer_dimensions[i],1)\n",
    "            \n",
    "            assert(self.network_parameters['Weights'+str(i)].shape == (self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1]))\n",
    "            assert(self.network_parameters['bias'+str(i)].shape == (self.layer_dimensions[i],1))\n",
    "            \n",
    "            self.gradient_network_parameters['dWeights'+str(i)] = np.zeros_like(self.network_parameters['Weights'+str(i)]) #np.zeros((self.layer_dimensions[i], self.layer_dimensions[i-1]))\n",
    "                                                                         #\n",
    "            self.gradient_network_parameters['dbias'+str(i)] = np.zeros_like(self.network_parameters['bias'+str(i)])#np.zeros((self.layer_dimensions[i],1))\n",
    "            \n",
    "            self.steep_parameters['dWeights'+str(i)] = np.zeros_like(self.network_parameters['Weights'+str(i)])\n",
    "            \n",
    "            self.steep_parameters['dbias'+str(i)] = np.zeros_like(self.network_parameters['bias'+str(i)])\n",
    "            \n",
    "            self.RMS_parameters['dWeights'+str(i)] = np.zeros_like(self.network_parameters['Weights'+str(i)])\n",
    "            \n",
    "            self.RMS_parameters['dbias'+str(i)] = np.zeros_like(self.network_parameters['bias'+str(i)])\n",
    "     \n",
    "    def batching(self, batching = False, batch_size = None):\n",
    "        '''\n",
    "        The NN_gradient_descent function helps to build and get the deep network working. \n",
    "        \n",
    "        inputs:\n",
    "        learning_rate : floating number\n",
    "        This is an hyper parameter which is used in gradient descent\n",
    "        '''\n",
    "        num_examples = self.training_data[0].shape[1]\n",
    "              \n",
    "        \n",
    "        if batching:\n",
    "            \n",
    "            training_input = self.training_data[0]\n",
    "            training_output = self.training_data[1]\n",
    "            \n",
    "            #mini_batches = []\n",
    "            \n",
    "            number_of_batches = int(num_examples/batch_size)\n",
    "            \n",
    "            for j in range(0,number_of_batches):\n",
    "                mini_train_input = training_input[:,(j*batch_size):((j+1)*batch_size)]\n",
    "                mini_train_output = training_output[:,(j*batch_size):((j+1)*batch_size)]\n",
    "                self.mini_batches.append((mini_train_input,mini_train_output))\n",
    "                \n",
    "            if num_examples % batch_size != 0:\n",
    "                mini_train_input = training_input[:,(number_of_batches*batch_size):]\n",
    "                mini_train_output = training_output[:,(number_of_batches*batch_size):]\n",
    "                self.mini_batches.append((mini_train_input,mini_train_output))\n",
    "        else:\n",
    "            \n",
    "            self.mini_batches = [self.training_data]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "\n",
    "    def sigmoid_derivative(self,Z):\n",
    "        return self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X]\n",
    "        for l in range(self.num_of_layers-1):\n",
    "\n",
    "            Z = np.dot(self.network_parameters['Weights'+str(l+1)], self.activations[l])+self.network_parameters['bias'+str(l+1)]\n",
    "\n",
    "            activated_Z = self.sigmoid(Z)\n",
    "\n",
    "            self.Z.append(Z)\n",
    "            self.activations.append(activated_Z)\n",
    "\n",
    "    def cost_function(self, error_method = 'MSE',test = False):\n",
    "        self.error_method = error_method\n",
    "        if test:\n",
    "            y_o = self.testing_data[1]\n",
    "        else:\n",
    "            y_o = self.training_data[1]\n",
    "        if error_method == 'MSE':\n",
    "            cost = 0.5*np.mean(np.square(self.activations[-1] - y_o))\n",
    "            self.cost_value = np.squeeze(cost)\n",
    "        \n",
    "\n",
    "    def cost_function_derivative(self, test=False):\n",
    "        if test:\n",
    "            y_o = self.testing_data[1]\n",
    "        else:\n",
    "            y_o = self.training_data[1]\n",
    "            \n",
    "        if self.error_method == 'MSE':\n",
    "            delC_delA = self.activations[-1] - y_o\n",
    "\n",
    "        self.loss = delC_delA\n",
    "\n",
    "    def back_propagation(self,X,Y):\n",
    "        \n",
    "        delC_delA = self.loss\n",
    "\n",
    "        #For Last Layer\n",
    "\n",
    "        #delC _delZ = delC_delA * delA_delZ\n",
    "        delC_delZ = delC_delA*self.sigmoid_derivative(self.Z[-1])\n",
    "        #delC_delB =  delC_delA * delA_delZ * delZ_delB\n",
    "        delC_delB = np.mean(delC_delZ, axis=1)\n",
    "        #delC_delW =  delC_delA * delA_delZ * delZ_delW\n",
    "        delC_delW = np.dot(delC_delZ,self.activations[-2].transpose())\n",
    "\n",
    "        self.gradient_network_parameters['dWeights'+str(self.num_of_layers-1)] = delC_delW\n",
    "        assert ( self.gradient_network_parameters['dWeights'+str(self.num_of_layers-1)].shape == self.network_parameters['Weights'+str(self.num_of_layers-1)].shape)\n",
    "        self.gradient_network_parameters['dbias'+str(self.num_of_layers-1)] = delC_delB.reshape(self.network_parameters['bias'+str(self.num_of_layers-1)].shape)\n",
    "        #assert ( self.gradient_network_parameters['dbias'+str(self.num_of_layers-1)].shape == self.network_parameters['bias'+str(self.num_of_layers-1)].shape)\n",
    "\n",
    "        for i in reversed(range(2,self.num_of_layers)):\n",
    "            delC_delA = np.dot(self.network_parameters['Weights'+str(i)].transpose(),delC_delZ)\n",
    "            delC_delZ = delC_delA*self.sigmoid_derivative(self.Z[i-2])\n",
    "            delC_delB = np.mean(delC_delZ, axis=1)\n",
    "            delC_delW = (1.0/X.shape[1])*np.dot(delC_delZ,self.activations[i-2].transpose())\n",
    "\n",
    "            self.gradient_network_parameters['dWeights'+str(i-1)] = delC_delW\n",
    "            assert ( self.gradient_network_parameters['dWeights'+str(i-1)].shape == self.network_parameters['Weights'+str(i-1)].shape)\n",
    "            self.gradient_network_parameters['dbias'+str(i-1)] = delC_delB.reshape(self.network_parameters['bias'+str(i-1)].shape)\n",
    "            assert ( self.gradient_network_parameters['dbias'+str(i-1)].shape == self.network_parameters['bias'+str(i-1)].shape)\n",
    "\n",
    "    def update_parameters_GD(self,learning_rate):\n",
    "            ''' Implementation of gradient descent method '''\n",
    "            for p in range(1,self.num_of_layers):\n",
    "\n",
    "                self.network_parameters['Weights'+str(p)] -= learning_rate*self.gradient_network_parameters['dWeights'+str(p)]\n",
    "\n",
    "                self.network_parameters['bias'+str(p)] -= learning_rate*self.gradient_network_parameters['dbias'+str(p)]\n",
    "\n",
    "    \n",
    "    def update_parameters_steepestGD(self,learning_rate,beta1):\n",
    "        '''Implementation of Steepest Gradient descent Method'''\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.steep_parameters['dWeights'+str(p)] = beta1*self.steep_parameters['dWeights'+str(p)] + (1-beta1)*self.gradient_network_parameters['dWeights'+str(p)]\n",
    "            self.steep_parameters['dbias'+str(p)] = beta1*self.steep_parameters['dbias'+str(p)] + (1-beta1)*self.gradient_network_parameters['dbias'+str(p)]\n",
    "            \n",
    "            self.network_parameters['Weights'+str(p)] -= learning_rate*self.steep_parameters['dWeights'+str(p)]\n",
    "            self.network_parameters['bias'+str(p)] -= learning_rate*self.steep_parameters['dbias'+str(p)]\n",
    "            \n",
    "    def update_parameters_RMSProp(self, learning_rate,beta2,epsilon=1e-8):\n",
    "        ''' Implementation of RMS Prop optimization'''\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.RMS_parameters['dWeights'+str(p)] = beta2*self.RMS_parameters['dWeights'+str(p)] + (1-beta2)*(self.gradient_network_parameters['dWeights'+str(p)])**2\n",
    "            self.RMS_parameters['dbias'+str(p)] = beta2*self.RMS_parameters['dbias'+str(p)] + (1-beta2)*(self.gradient_network_parameters['dbias'+str(p)])**2\n",
    "            \n",
    "            self.network_parameters['Weights'+str(p)] -= (learning_rate*self.gradient_network_parameters['dWeights'+str(p)])/(np.sqrt(self.RMS_parameters['dWeights'+str(p)])+epsilon)\n",
    "            self.network_parameters['bias'+str(p)] -= (learning_rate*self.gradient_network_parameters['dbias'+str(p)])/(np.sqrt(self.RMS_parameters['dbias'+str(p)])+epsilon)\n",
    "        \n",
    "    def update_parameters_Adam(self,learning_rate,beta1, beta2, epsilon=1e-8):\n",
    "        '''Implementation of Adaptive Moment Estimation(Adam) optimization'''\n",
    "        \n",
    "        temp_steep_parameters, temp_RMS_parameters = {}, {}\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.steep_parameters['dWeights'+str(p)] = beta1*self.steep_parameters['dWeights'+str(p)] + (1-beta1)*self.gradient_network_parameters['dWeights'+str(p)]\n",
    "            self.steep_parameters['dbias'+str(p)] = beta1*self.steep_parameters['dbias'+str(p)] + (1-beta1)*self.gradient_network_parameters['dbias'+str(p)]\n",
    "            \n",
    "            temp_steep_parameters['dWeights'+str(p)] = self.steep_parameters['dWeights'+str(p)]/(1-beta1**p)\n",
    "            temp_steep_parameters['dbias'+str(p)] = self.steep_parameters['dbias'+str(p)]/(1-beta1**(p))\n",
    "            \n",
    "            self.RMS_parameters['dWeights'+str(p)] = beta2*self.RMS_parameters['dWeights'+str(p)] + (1-beta2)*(self.gradient_network_parameters['dWeights'+str(p)])**2\n",
    "            self.RMS_parameters['dbias'+str(p)] = beta2*self.RMS_parameters['dbias'+str(p)] + (1-beta2)*(self.gradient_network_parameters['dbias'+str(p)])**2\n",
    "            \n",
    "            temp_RMS_parameters['dWeights'+str(p)] = self.RMS_parameters['dWeights'+str(p)]/(1-beta2**p)\n",
    "            temp_RMS_parameters['dbias'+str(p)] = self.RMS_parameters['dbias'+str(p)]/(1-beta2**p)\n",
    "            \n",
    "            self.network_parameters['Weights'+str(p)] -= (learning_rate*temp_steep_parameters['dWeights'+str(p)])/(np.sqrt(temp_RMS_parameters['dWeights'+str(p)])+epsilon)\n",
    "            self.network_parameters['bias'+str(p)] -= (learning_rate*temp_steep_parameters['dbias'+str(p)])/(np.sqrt(temp_RMS_parameters['dbias'+str(p)])+epsilon)\n",
    "        \n",
    "    def NN_model(self, epochs, learning_rate, beta1=None, beta2=None, batching=False, batch_size = None,  error_method = 'MSE', optimizer='GD'):\n",
    "        \n",
    "        ''' Deep neural network model'''\n",
    "        \n",
    "        self.network_parameters_initialization()\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        for iteration in range(epochs):\n",
    "            \n",
    "            #Batching\n",
    "            self.batching(batching=False, batch_size=None)\n",
    "            \n",
    "            #Traversing through Mini Batches:\n",
    "            for mini_batch in self.mini_batches:\n",
    "                \n",
    "                mini_batch_X, mini_batch_Y = mini_batch\n",
    "                \n",
    "                #Forward Prop\n",
    "                self.forward_propagation(mini_batch_X)\n",
    "                \n",
    "                #Loss calculation\n",
    "                self.cost_function(error_method)\n",
    "                self.cost_function_derivative()\n",
    "\n",
    "                if iteration%(epochs/10) == 0:\n",
    "                    self.cost.append(np.squeeze(np.mean(self.loss)))\n",
    "                    print('The cost after iteration: ', iteration, 'is :', self.cost_value)#np.squeeze(np.mean(self.loss)))\n",
    "                #Back Prop\n",
    "                self.back_propagation(mini_batch_X,mini_batch_Y)\n",
    "                \n",
    "                if self.optimizer == 'GD':\n",
    "                    #Updating parameters with Gradient Descent\n",
    "                    self.update_parameters_GD(learning_rate)\n",
    "                \n",
    "                elif self.optimizer == 'Steepest GD':\n",
    "                    #Updating parameters with Steepest Gradient\n",
    "                    self.update_parameters_steepestGD(learning_rate, beta1)\n",
    "                    \n",
    "                elif self.optimizer == 'RMSProp':\n",
    "                    self.update_parameters_RMSProp(learning_rate, beta2)\n",
    "                    \n",
    "                elif self.optimizer == 'adam':\n",
    "                    #Updating parameters with Adam optimization\n",
    "                    self.update_parameters_Adam(learning_rate, beta1, beta2)\n",
    "                    \n",
    "                \n",
    "    \n",
    "                \n",
    "        #prediction\n",
    "        prediction_train = self.training_data[0]\n",
    "        self.forward_propagation(prediction_train)\n",
    "        \n",
    "        #Evaluation\n",
    "        self.evaluate()\n",
    "        \n",
    "        \n",
    "    def evaluate(self):\n",
    "        prediction_test = self.testing_data[0]\n",
    "        self.forward_propagation(prediction_test)\n",
    "        \n",
    "        self.cost_function(self.error_method, test=True)\n",
    "        print('The cost in Testing is: ', self.cost_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import numpy as np\n",
    "data = np.loadtxt('log.txt')\n",
    "data1 = np.zeros((data.shape[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def NN_model(self, epochs, learning_rate, beta1 , batching=False, batch_size = None,  error_method = 'MSE', optimizer='GD')\n",
    "hidden_layer_dims = [15,15]\n",
    "model = my_neural_network(hidden_layer_dims)\n",
    "file_name = 'log.txt'\n",
    "model.load_dataset(file_name)\n",
    "model.test_train_split()\n",
    "model.NN_model(100000, 0.01,beta2=0.88, batching = True, batch_size= 64,optimizer = 'RMSProp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(model.cost[1:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting testing output v/s predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.testing_data[1],model.activations[-1].reshape(model.testing_data[1].shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.testing_data[1], 'r--')\n",
    "plt.plot(model.activations[-1].reshape(model.testing_data[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "X = model.testing_data[0]\n",
    "X1 = X[0,:]\n",
    "X2 = X[1,:]\n",
    "fig1 = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.axes(projection='3d')\n",
    "ax1.plot3D(X1,X2,model.testing_data[1], 'r--')\n",
    "Y = model.activations[-1].reshape(model.testing_data[1].shape)\n",
    "ax1.plot3D(X1,X2,Y)#, 'r--')\n",
    "#X1.shape, X2.shape,model.testing_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
