{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_neural_network:\n",
    "    \n",
    "    def __init__(self, hidden_layer_plan):\n",
    "        \n",
    "        '''\n",
    "        Initialization function.\n",
    "        \n",
    "        input: \n",
    "        layer_plan : data_type = list\n",
    "        \n",
    "        List cointaing the number of neurons in every layers for the neural network. The first and last elements of\n",
    "        layer_plan corresponds to number of input features and the output value. \n",
    "        The rest of the elements between them corresponds to the neurons in hidden layers.\n",
    "        More the Hidden Layers implies deeper the network!\n",
    "        '''\n",
    "        \n",
    "        #dataset\n",
    "        self.dataset = None\n",
    "        \n",
    "        #input and output data\n",
    "        self.input_data = None\n",
    "        self.output_data = None\n",
    "        self.predicted_output =None\n",
    "        \n",
    "        #train and test data\n",
    "        self.training_data = None\n",
    "        self.testing_data = None\n",
    "        \n",
    "        #layer details\n",
    "        self.hidden_layer_plan = hidden_layer_plan\n",
    "        self.layer_dimensions = None\n",
    "        self.num_of_layers = None\n",
    "        \n",
    "        #Batching\n",
    "        self.mini_batches = []\n",
    "        \n",
    "        #Forward Activation\n",
    "        self.activations, self.Z = [], []\n",
    "        \n",
    "        #Backward Activation\n",
    "        self.dActivations = []\n",
    "        \n",
    "        #Error_method and loss\n",
    "        self.error_method = None\n",
    "        self.loss = None\n",
    "        self.cost = []\n",
    "        self.cost_value = None\n",
    "        \n",
    "        #Network parameters\n",
    "        self.network_weights, self.gradient_weights,self.momentum_weights, self.rms_weights = [],[],[],[]\n",
    "        self.network_bias, self.gradient_bias,self.momentum_bias, self.rms_bias = [],[],[],[]\n",
    "        #optimizer\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        ''' Representative function to welcome the user for collaborating with the new AI trainer'''\n",
    "        return 'Hello there!! I am your new AI trainer......!'\n",
    "    \n",
    "    \n",
    "    def load_dataset(self, file):\n",
    "        ''' \n",
    "        To load dataset \n",
    "        \n",
    "        input:\n",
    "        file : data_type: string\n",
    "        File name consisting the inputs and outputs. The last column in the file is the output.\n",
    "        '''\n",
    "        self.dataset = np.loadtxt(file)\n",
    "        self.input_data = self.dataset[:,:-1]\n",
    "        self.output_data = self.dataset[:,-1]\n",
    "        \n",
    "        #Normalization\n",
    "        max_inputs = np.amax(self.input_data, axis=0)\n",
    "        min_inputs = np.amin(self.input_data, axis=0)\n",
    "        diff_inputs = max_inputs - min_inputs\n",
    "        self.input_data = np.divide((self.input_data - min_inputs),diff_inputs)\n",
    "        \n",
    "        max_outputs = np.amax(self.output_data, axis=0)\n",
    "        min_outputs = np.amin(self.output_data, axis=0)\n",
    "        diff_outputs = max_outputs - min_outputs\n",
    "        self.output_data = np.divide((self.output_data - min_outputs), diff_outputs)\n",
    "        \n",
    "        #Layer planning\n",
    "        self.layer_dimensions = [self.input_data.shape[1]]+self.hidden_layer_plan+[1] #1 corresponds to columns in O/P\n",
    "        self.num_of_layers = len(self.layer_dimensions)\n",
    "        \n",
    "    def test_train_split(self, split = 0.7):\n",
    "        '''\n",
    "        This function is utilized to segragate the complete data into training and testing data.\n",
    "        '''\n",
    "        \n",
    "        n_total = int(self.dataset.shape[0])\n",
    "        n_train = int(split*n_total)\n",
    "        \n",
    "        mask = np.zeros((n_total), dtype=bool)\n",
    "        mask[:n_train] = True\n",
    "        \n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        X_train = self.input_data[mask]\n",
    "        Y_train = self.output_data[mask]\n",
    "        \n",
    "        X_test = self.input_data[~mask]\n",
    "        Y_test = self.output_data[~mask]\n",
    "        \n",
    "        self.training_data = (X_train.transpose(), Y_train)\n",
    "        self.testing_data = (X_test.transpose(), Y_test)\n",
    "    \n",
    "    def network_parameters_initialization(self):\n",
    "        np.random.seed(1)\n",
    "        for i in range(1,self.num_of_layers):\n",
    "            self.network_weights.append(np.random.randn(self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1]))\n",
    "            self.network_bias.append(np.random.randn(self.layer_dimensions[i],1))\n",
    "            \n",
    "            assert(self.network_weights[i-1].shape == (self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1]))\n",
    "            assert(self.network_bias[i-1].shape == (self.layer_dimensions[i],1))\n",
    "            \n",
    "            self.gradient_weights.append(np.zeros_like(self.network_weights[i-1])) #np.zeros((self.layer_dimensions[i], self.layer_dimensions[i-1]))\n",
    "                                                                         #\n",
    "            self.gradient_bias.append(np.zeros_like(self.network_bias[i-1]))#np.zeros((self.layer_dimensions[i],1))\n",
    "            \n",
    "            self.momentum_weights.append(np.zeros_like(self.network_weights[i-1]))\n",
    "            \n",
    "            self.momentum_bias.append(np.zeros_like(self.network_bias[i-1]))\n",
    "            \n",
    "            self.rms_weights.append(np.zeros_like(self.network_weights[i-1]))\n",
    "            \n",
    "            self.rms_bias.append(np.zeros_like(self.network_bias[i-1]))\n",
    "     \n",
    "    def batching(self, batching = False, batch_size = None):\n",
    "        '''\n",
    "        The NN_gradient_descent function helps to build and get the deep network working. \n",
    "        \n",
    "        inputs:\n",
    "        learning_rate : floating number\n",
    "        This is an hyper parameter which is used in gradient descent\n",
    "        '''\n",
    "        num_examples = self.training_data[0].shape[1]\n",
    "              \n",
    "        \n",
    "        if batching:\n",
    "            \n",
    "            training_input = self.training_data[0]\n",
    "            training_output = self.training_data[1]\n",
    "            \n",
    "            #mini_batches = []\n",
    "            \n",
    "            number_of_batches = int(num_examples/batch_size)\n",
    "            \n",
    "            for j in range(0,number_of_batches):\n",
    "                mini_train_input = training_input[:,(j*batch_size):((j+1)*batch_size)]\n",
    "                mini_train_output = training_output[:,(j*batch_size):((j+1)*batch_size)]\n",
    "                self.mini_batches.append((mini_train_input,mini_train_output))\n",
    "                \n",
    "            if num_examples % batch_size != 0:\n",
    "                mini_train_input = training_input[:,(number_of_batches*batch_size):]\n",
    "                mini_train_output = training_output[:,(number_of_batches*batch_size):]\n",
    "                self.mini_batches.append((mini_train_input,mini_train_output))\n",
    "        else:\n",
    "            \n",
    "            self.mini_batches = [self.training_data]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid(self,Z):\n",
    "        return 1./(1+np.exp(-Z))\n",
    "\n",
    "    def sigmoid_derivative(self,Z):\n",
    "        return self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X]\n",
    "        for l in range(self.num_of_layers-1):\n",
    "\n",
    "            Z = np.dot(self.network_weights[l], self.activations[l])+self.network_bias[l]\n",
    "\n",
    "            activated_Z = self.sigmoid(Z)\n",
    "\n",
    "            self.Z.append(Z)\n",
    "            self.activations.append(activated_Z)\n",
    "\n",
    "    def cost_function(self, error_method = 'MSE',test = False):\n",
    "        self.error_method = error_method\n",
    "        if test:\n",
    "            y_o = self.testing_data[1]\n",
    "        else:\n",
    "            y_o = self.training_data[1]\n",
    "        if error_method == 'MSE':\n",
    "            cost = 0.5*np.mean(np.square(self.activations[-1] - y_o))\n",
    "            self.cost_value = np.squeeze(cost)\n",
    "        \n",
    "\n",
    "    def cost_function_derivative(self, test=False):\n",
    "        if test:\n",
    "            y_o = self.testing_data[1]\n",
    "        else:\n",
    "            y_o = self.training_data[1]\n",
    "            \n",
    "        if self.error_method == 'MSE':\n",
    "            delC_delA = self.activations[-1] - y_o\n",
    "\n",
    "        self.loss = delC_delA\n",
    "\n",
    "    def back_propagation(self,X,Y):\n",
    "        \n",
    "        delC_delA = self.loss\n",
    "\n",
    "        #For Last Layer\n",
    "\n",
    "        #delC _delZ = delC_delA * delA_delZ\n",
    "        delC_delZ = delC_delA*self.sigmoid_derivative(self.Z[-1])\n",
    "        #delC_delB =  delC_delA * delA_delZ * delZ_delB\n",
    "        delC_delB = np.mean(delC_delZ, axis=1,keepdims = True)\n",
    "        #delC_delW =  delC_delA * delA_delZ * delZ_delW\n",
    "        delC_delW = (1.0/X.shape[1])*np.dot(delC_delZ,self.activations[-2].transpose())\n",
    "\n",
    "        self.gradient_weights[-1] = delC_delW\n",
    "        assert ( self.gradient_weights[-1].shape == self.network_weights[-1].shape)\n",
    "        self.gradient_bias[-1] = delC_delB#.reshape(self.network_bias[-1].shape)\n",
    "        #assert ( self.gradient_network_parameters['dbias'+str(self.num_of_layers-1)].shape == self.network_parameters['bias'+str(self.num_of_layers-1)].shape)\n",
    "\n",
    "        for i in reversed(range(2,self.num_of_layers)):\n",
    "            delC_delA = np.dot(self.network_weights[i-1].transpose(),delC_delZ)\n",
    "            delC_delZ = delC_delA*self.sigmoid_derivative(self.Z[i-2])\n",
    "            delC_delB = np.mean(delC_delZ, axis=1)\n",
    "            delC_delW = (1.0/X.shape[1])*np.dot(delC_delZ,self.activations[i-2].transpose())\n",
    "\n",
    "            self.gradient_weights[i-2] = delC_delW\n",
    "            assert ( self.gradient_weights[i-2].shape == self.network_weights[i-2].shape)\n",
    "            self.gradient_bias[i-2] = delC_delB.reshape(self.network_bias[i-2].shape)\n",
    "            assert ( self.gradient_bias[i-2].shape == self.network_bias[i-2].shape)\n",
    "\n",
    "    def update_parameters_GD(self,learning_rate):\n",
    "            ''' Implementation of gradient descent method '''\n",
    "            for p in range(1,self.num_of_layers):\n",
    "\n",
    "                self.network_weights[p-1] -= learning_rate*self.gradient_weights[p-1]\n",
    "\n",
    "                self.network_bias[p-1] -= learning_rate*self.gradient_bias[p-1]\n",
    "\n",
    "    \n",
    "    def update_parameters_steepestGD(self,learning_rate,beta1):\n",
    "        '''Implementation of Steepest Gradient descent Method'''\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.momentum_weights[p-1] = beta1*self.momentum_weights[p-1] + (1-beta1)*self.gradient_weights[p-1]\n",
    "            self.momentum_bias[p-1] = beta1*self.momentum_bias[p-1] + (1-beta1)*self.gradient_bias[p-1]\n",
    "            \n",
    "            self.network_weights[p-1] -= learning_rate*self.momentum_weights[p-1]\n",
    "            self.network_bias[p-1] -= learning_rate*self.momentum_bias[p-1]\n",
    "            \n",
    "    \n",
    "    def update_parameters_Adam(self,learning_rate,beta1, beta2, epsilon=1e-8):\n",
    "        '''Implementation of Adaptive Moment Estimation(Adam) optimization'''\n",
    "        \n",
    "        temp_momentum_weights, temp_momentum_bias, temp_rms_weights, temp_rms_bias = [],[],[],[]\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.momentum_weights[p-1] = beta1*self.momentum_weights[p-1] + (1-beta1)*self.gradient_weights[p-1]\n",
    "            self.momentum_bias[p-1] = beta1*self.momentum_bias[p-1] + (1-beta1)*self.gradient_bias[p-1]\n",
    "            \n",
    "            temp_momentum_weights[p-1].append(self.momentum_weights[p-1]/(1-beta1**p))\n",
    "            temp_momentum_bias.append(self.momentum_bias[p-1]/(1-beta1**(p)))\n",
    "            \n",
    "            self.rms_weights[p-1]= beta2*self.rms_weights[p-1] + (1-beta2)*(self.gradient_weights[p-1])**2\n",
    "            self.rms_bias[p-1] = beta2*self.rms_bias[p-1] + (1-beta2)*(self.gradient_bias[p-1])**2\n",
    "            \n",
    "            temp_rms_weights.append(self.rms_weights[p-1]/(1-beta2**p))\n",
    "            temp_rms_bias.append(self.rms_bias[p-1]/(1-beta2**p))\n",
    "            \n",
    "            self.network_weights[p-1] -= (learning_rate*temp_momentum_weights[p-1])/(np.sqrt(temp_rms_weights[p-1])+epsilon)\n",
    "            self.network_bias[p-1] -= (learning_rate*temp_momentum_bias[p-1])/(np.sqrt(temp_rms_bias[p-1])+epsilon)\n",
    "        \n",
    "    def NN_model(self, epochs, learning_rate, beta1=None, beta2=None, batching=False, batch_size = None, error_method = 'MSE', optimizer = 'GD'):\n",
    "        \n",
    "        ''' Deep neural network model'''\n",
    "        \n",
    "        self.network_parameters_initialization()\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        for iteration in range(epochs):\n",
    "            \n",
    "            #Batching\n",
    "            self.batching(batching=False, batch_size=None)\n",
    "            \n",
    "            #Traversing through Mini Batches:\n",
    "            for mini_batch in self.mini_batches:\n",
    "                \n",
    "                mini_batch_X, mini_batch_Y = mini_batch\n",
    "                \n",
    "                #Forward Prop\n",
    "                self.forward_propagation(mini_batch_X)\n",
    "                \n",
    "                #Loss calculation\n",
    "                self.cost_function(error_method)\n",
    "                self.cost_function_derivative()\n",
    "\n",
    "                if iteration%(epochs/10) == 0:\n",
    "                    self.cost.append(np.squeeze(np.mean(self.loss)))\n",
    "                    print('The cost after iteration: ', iteration, 'is :', self.cost_value)#np.squeeze(np.mean(self.loss)))\n",
    "                #Back Prop\n",
    "                self.back_propagation(mini_batch_X,mini_batch_Y)\n",
    "                \n",
    "                #if self.optimizer == 'GD':\n",
    "                    #Updating parameters with Gradient Descent\n",
    "                    #self.update_parameters_GD(learning_rate)\n",
    "                \n",
    "                #elif self.optimizer == 'Steepest GD':\n",
    "                    #Updating parameters with Steepest Gradient\n",
    "                    #self.update_parameters_steepestGD(learning_rate, beta1)\n",
    "                    \n",
    "                #elif self.optimizer == 'adam':\n",
    "                    #Updating parameters with Adam optimization\n",
    "                    #self.update_parameters_Adam(learning_rate, beta1, beta2)\n",
    "                    \n",
    "                \n",
    "    \n",
    "                \n",
    "        #prediction\n",
    "       # prediction_train = self.training_data[0]\n",
    "        #self.forward_propagation(prediction_train)\n",
    "        \n",
    "        #Evaluation\n",
    "        #self.evaluate()\n",
    "        \n",
    "        \n",
    "    def evaluate(self):\n",
    "        prediction_test = self.testing_data[0]\n",
    "        self.forward_propagation(prediction_test)\n",
    "        \n",
    "        self.cost_function(self.error_method, test=True)\n",
    "        print('The cost in Testing is: ', self.cost_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import numpy as np\n",
    "data = np.loadtxt('log.txt')\n",
    "data1 = np.zeros((data.shape[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after iteration:  0 is : 0.16006721037650062\n"
     ]
    }
   ],
   "source": [
    "#def NN_model(self, epochs, learning_rate, beta1 , batching=False, batch_size = None,  error_method = 'MSE', optimizer='GD')\n",
    "hidden_layer_dims = [2]\n",
    "model = my_neural_network(hidden_layer_dims)\n",
    "file_name = 'log.txt'\n",
    "model.load_dataset(file_name)\n",
    "model.test_train_split()\n",
    "model.NN_model(1, 0.01,beta1=0.8, batching = True, batch_size= 64,optimizer = 'Steepest GD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 1.62434536, -0.61175641],\n",
       "         [-0.52817175, -1.07296862]]), array([[ 1.74481176, -0.7612069 ]])],\n",
       " [array([[ 0.86540763],\n",
       "         [-2.3015387 ]]), array([[0.3190391]])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network_weights , model.network_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00824816,  0.00932826],\n",
       "        [-0.00098294, -0.00084458]]), array([[0.04929209, 0.00280366]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gradient_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = model.training_data[0]\n",
    "x = inp[:,35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = np.array([1.62434536+ 1e-7, -0.61175641])# + 1e-7\n",
    "am = np.array([1.62434536- 1e-7, -0.61175641])# - 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1p = np.dot(ap,inp) + np.array([[ 0.86540763],\n",
    "         [-2.3015387 ]])\n",
    "z1m = np.dot(am,inp) + np.array([[ 0.86540763],\n",
    "         [-2.3015387 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1p= (1+np.exp(-z1p))**-1\n",
    "a1m= (1+np.exp(-z1m))**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 =np.array([[ 1.74481176, -0.7612069 ]])\n",
    "b2 = np.array([[0.3190391]])\n",
    "z2p=np.dot(W2,a1p)+b2\n",
    "z2m=np.dot(W2,a1m)+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2p= (1+np.exp(-z2p))**-1\n",
    "a2m= (1+np.exp(-z2m))**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp=model.training_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp=np.mean((a2p-y)**2)\n",
    "jm=np.mean((a2m-y)**2)\n",
    "grad = (jp-jm)/(2e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010519107807027694"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0022709478070276953"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.010519107807027694- 0.00824816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018767267807027692"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.010519107807027694+ 0.00824816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12100577614059005"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0022709478070276953/0.018767267807027692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.network_weights:\n",
    "    for j in range(i.shape[0]):\n",
    "        for k in range(i.shape[1]):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 700 into shape (300,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cc90fa357e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 700 into shape (300,)"
     ]
    }
   ],
   "source": [
    "plt.scatter(model.testing_data[1],model.activations[-1].reshape(model.testing_data[1].shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.testing_data[1], 'r--')\n",
    "plt.plot(model.activations[-1].reshape(model.testing_data[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "X = model.testing_data[0]\n",
    "X1 = X[0,:]\n",
    "X2 = X[1,:]\n",
    "fig1 = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.axes(projection='3d')\n",
    "ax1.plot3D(X1,X2,model.testing_data[1], 'r--')\n",
    "Y = model.activations[-1].reshape(model.testing_data[1].shape)\n",
    "ax1.plot3D(X1,X2,Y)#, 'r--')\n",
    "#X1.shape, X2.shape,model.testing_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
