{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_neural_network:\n",
    "    \n",
    "    def __init__(self, hidden_layer_plan):\n",
    "        \n",
    "        '''\n",
    "        Initialization function.\n",
    "        \n",
    "        input: \n",
    "        layer_plan : data_type = list\n",
    "        \n",
    "        List cointaing the number of neurons in every layers for the neural network. The first and last elements of\n",
    "        layer_plan corresponds to number of input features and the output value. \n",
    "        The rest of the elements between them corresponds to the neurons in hidden layers.\n",
    "        More the Hidden Layers implies deeper the network!\n",
    "        '''\n",
    "        \n",
    "        #dataset\n",
    "        '''Object to store the name of the dataset'''\n",
    "        self.dataset = None\n",
    "        \n",
    "        #input and output data\n",
    "        '''Objects to store the input and the coutput data. An attribute is additionally set to store the calculated output from the neural network'''\n",
    "        self.input_data = None\n",
    "        self.output_data = None\n",
    "        self.predicted_output =None\n",
    "        \n",
    "        #train and test data\n",
    "        '''Objects to store the Training data and Testing data. This is doen during the training and test data split'''\n",
    "        self.training_data = None\n",
    "        self.testing_data = None\n",
    "        \n",
    "        #layer details\n",
    "        '''The attributes defining the deep network that is the number of hidden layers and its respective neurons '''\n",
    "        self.hidden_layer_plan = hidden_layer_plan\n",
    "        self.layer_dimensions = None\n",
    "        self.num_of_layers = None\n",
    "        \n",
    "        #Batching\n",
    "        ''' List to store the mini batches'''\n",
    "        self.mini_batches = []\n",
    "        \n",
    "        #Forward Activation\n",
    "        ''' Objects to utilize during Forward Propagation'''\n",
    "        self.activation_function_name = None\n",
    "        self.activations, self.Z = [], [] # Lists to store the Activation outputs and their respective Linear Functions\n",
    "        \n",
    "        #Backward Activation\n",
    "        self.dActivations = []\n",
    "        \n",
    "        #Error_method and loss\n",
    "        self.error_method = None\n",
    "        self.loss = None\n",
    "        self.cost = []\n",
    "        self.cost_value = None\n",
    "        \n",
    "        #Network parameters\n",
    "        self.network_weights, self.gradient_weights,self.momentum_weights, self.rms_weights = [],[],[],[]\n",
    "        self.network_bias, self.gradient_bias,self.momentum_bias, self.rms_bias = [],[],[],[]\n",
    "        #optimizer\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        ''' Representative function to welcome the user for collaborating with the new AI trainer'''\n",
    "        return 'Hello there!! I am your new AI trainer......!'\n",
    "    \n",
    "    \n",
    "    def load_dataset(self, file):\n",
    "        ''' \n",
    "        To load dataset \n",
    "        \n",
    "        input:\n",
    "        file : data_type: string\n",
    "        File name consisting the inputs and outputs. The last column in the file is the output.\n",
    "        '''\n",
    "        self.dataset = np.loadtxt(file)\n",
    "        self.input_data = self.dataset[:,:-1]\n",
    "        self.output_data = self.dataset[:,-1]\n",
    "        \n",
    "        #Normalization\n",
    "        '''Normalization helps to modulate teh data such that teh values lies along 0 and 1. This is helpful when using sigmoid functions or any activation functions'''\n",
    "        max_inputs = np.amax(self.input_data, axis=0)\n",
    "        min_inputs = np.amin(self.input_data, axis=0)\n",
    "        diff_inputs = max_inputs - min_inputs\n",
    "        self.input_data = np.divide((self.input_data - min_inputs),diff_inputs)\n",
    "        \n",
    "        max_outputs = np.amax(self.output_data, axis=0)\n",
    "        min_outputs = np.amin(self.output_data, axis=0)\n",
    "        diff_outputs = max_outputs - min_outputs\n",
    "        self.output_data = np.divide((self.output_data - min_outputs), diff_outputs)\n",
    "        \n",
    "        #Layer planning\n",
    "        self.layer_dimensions = [self.input_data.shape[1]]+self.hidden_layer_plan+[1] #1 corresponds to columns in O/P\n",
    "        self.num_of_layers = len(self.layer_dimensions)\n",
    "        \n",
    "    def test_train_split(self, split = 0.7):\n",
    "        '''\n",
    "        This function is utilized to segragate the complete data into training and testing data.\n",
    "        '''\n",
    "        \n",
    "        n_total = int(self.dataset.shape[0])\n",
    "        n_train = int(split*n_total)\n",
    "        \n",
    "        mask = np.zeros((n_total), dtype=bool)\n",
    "        mask[:n_train] = True\n",
    "        \n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        X_train = self.input_data[mask]\n",
    "        Y_train = self.output_data[mask]\n",
    "        \n",
    "        X_test = self.input_data[~mask]\n",
    "        Y_test = self.output_data[~mask]\n",
    "        \n",
    "        self.training_data = (X_train.transpose(), Y_train)\n",
    "        self.testing_data = (X_test.transpose(), Y_test)\n",
    "    \n",
    "    def network_parameters_initialization(self):\n",
    "        ''' Helps to initialize the network parameters '''\n",
    "        np.random.seed(1)\n",
    "        for i in range(1,self.num_of_layers):\n",
    "            self.network_weights.append(np.random.randn(self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1]))\n",
    "            self.network_bias.append(np.random.randn(self.layer_dimensions[i],1))\n",
    "            \n",
    "            assert(self.network_weights[i-1].shape == (self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1]))\n",
    "            assert(self.network_bias[i-1].shape == (self.layer_dimensions[i],1))\n",
    "            \n",
    "            self.gradient_weights.append(np.zeros_like(self.network_weights[i-1])) #np.zeros((self.layer_dimensions[i], self.layer_dimensions[i-1]))\n",
    "                                                                         #\n",
    "            self.gradient_bias.append(np.zeros_like(self.network_bias[i-1]))#np.zeros((self.layer_dimensions[i],1))\n",
    "            \n",
    "            self.momentum_weights.append(np.zeros_like(self.network_weights[i-1]))\n",
    "            \n",
    "            self.momentum_bias.append(np.zeros_like(self.network_bias[i-1]))\n",
    "            \n",
    "            self.rms_weights.append(np.zeros_like(self.network_weights[i-1]))\n",
    "            \n",
    "            self.rms_bias.append(np.zeros_like(self.network_bias[i-1]))\n",
    "     \n",
    "    def batching(self, batching = False, batch_size = None):\n",
    "        '''\n",
    "        The NN_gradient_descent function helps to build and get the deep network working. \n",
    "        \n",
    "        inputs:\n",
    "        learning_rate : floating number\n",
    "        This is an hyper parameter which is used in gradient descent\n",
    "        '''\n",
    "        num_examples = self.training_data[0].shape[1]\n",
    "              \n",
    "        \n",
    "        if batching:\n",
    "            \n",
    "            training_input = self.training_data[0]\n",
    "            training_output = self.training_data[1]\n",
    "            \n",
    "            #mini_batches = []\n",
    "            \n",
    "            number_of_batches = int(num_examples/batch_size)\n",
    "            \n",
    "            for j in range(0,number_of_batches):\n",
    "                mini_train_input = training_input[:,(j*batch_size):((j+1)*batch_size)]\n",
    "                mini_train_output = training_output[:,(j*batch_size):((j+1)*batch_size)]\n",
    "                self.mini_batches.append((mini_train_input,mini_train_output))\n",
    "                \n",
    "            if num_examples % batch_size != 0:\n",
    "                mini_train_input = training_input[:,(number_of_batches*batch_size):]\n",
    "                mini_train_output = training_output[:,(number_of_batches*batch_size):]\n",
    "                self.mini_batches.append((mini_train_input,mini_train_output))\n",
    "        else:\n",
    "            \n",
    "            self.mini_batches = [self.training_data]\n",
    "\n",
    "\n",
    "    def activation_function(self,Z):\n",
    "        '''In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. [Wikipedia]'''\n",
    "        \n",
    "        if self.activation_function_name == \"sigmoid\":\n",
    "            return self.sigmoid(Z)\n",
    "        elif self.activation_function_name == \"relu\":\n",
    "            return self.relu(Z)\n",
    "        elif self.activation_function_name == \"tanh\":\n",
    "            return self.tanh(Z)\n",
    "        \n",
    "    def activation_function_derivative(self,Z):\n",
    "        ''' Functions to call the corresponding derivatives of the activation functions'''\n",
    "        if self.activation_function_name == \"sigmoid\":\n",
    "            return self.sigmoid_derivative(Z)\n",
    "        elif self.activation_function_name == \"relu\":\n",
    "            return self.relu_derivative(Z)\n",
    "        elif self.activation_function_name == \"tanh\":\n",
    "            return self.tanh_derivative(Z)\n",
    "        \n",
    "\n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "\n",
    "    def sigmoid_derivative(self,Z):\n",
    "        return self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
    "    \n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z)\n",
    "    \n",
    "    def relu_derivative(self,Z):\n",
    "        dz = np.ones_like(Z)\n",
    "        dz[Z<0] = 0\n",
    "        return dz\n",
    "    def tanh(self,Z):\n",
    "        return np.tanh(Z)\n",
    "    \n",
    "    def tanh_derivative(self,Z):\n",
    "        return np.reciprocal(np.square(np.cosh(Z))) #sech^2(Z)\n",
    "\n",
    "    def forward_propagation(self, X, check_w = None, check_b = None):\n",
    "        self.activations = [X]\n",
    "        #This is useful during gradient checking\n",
    "        if check_w == None and check_b == None:\n",
    "            weights = self.network_weights\n",
    "            bias = self.network_bias\n",
    "        else:\n",
    "            weights = check_w\n",
    "            bias = check_b\n",
    "            \n",
    "        for l in range(self.num_of_layers-1):\n",
    "\n",
    "            Z = np.dot(weights[l], self.activations[l]) + bias[l]\n",
    "\n",
    "            activated_Z = self.activation_function(Z)\n",
    "\n",
    "            self.Z.append(Z)\n",
    "            self.activations.append(activated_Z)\n",
    "\n",
    "    def cost_function(self, error_method = 'MSE',test = False):\n",
    "        ''' Using cost function helps to determine the amount of loss created using the parameters in forward propagation. Minimizing the \n",
    "        cost function is the main objective\n",
    "        '''\n",
    "        self.error_method = error_method\n",
    "        if test:\n",
    "            y_o = self.testing_data[1]\n",
    "        else:\n",
    "            y_o = self.training_data[1]\n",
    "        \n",
    "        if error_method == 'MSE':\n",
    "            cost = 0.5*np.mean(np.square(self.activations[-1] - y_o))\n",
    "            self.cost_value = np.squeeze(cost)\n",
    "        \n",
    "\n",
    "    def cost_function_derivative(self, test=False):\n",
    "        if test:\n",
    "            y_o = self.testing_data[1]\n",
    "        else:\n",
    "            y_o = self.training_data[1]\n",
    "            \n",
    "        if self.error_method == 'MSE':\n",
    "            delC_delA = self.activations[-1] - y_o\n",
    "\n",
    "        self.loss = delC_delA\n",
    "\n",
    "    def back_propagation(self,X,Y):\n",
    "        ''' To find the gradient, Back propagation is used. Gradients help to find the flatness i.e, minima'''\n",
    "        \n",
    "        delC_delA = self.loss\n",
    "\n",
    "        #For Last Layer\n",
    "\n",
    "        #delC _delZ = delC_delA * delA_delZ\n",
    "        delC_delZ = delC_delA*self.activation_function_derivative(self.Z[-1])\n",
    "        #delC_delB =  delC_delA * delA_delZ * delZ_delB\n",
    "        delC_delB = np.mean(delC_delZ, axis=1,keepdims = True)\n",
    "        #delC_delW =  delC_delA * delA_delZ * delZ_delW\n",
    "        delC_delW = (1.0/X.shape[1])*np.dot(delC_delZ,self.activations[-2].transpose())\n",
    "\n",
    "        self.gradient_weights[-1] = delC_delW\n",
    "        assert ( self.gradient_weights[-1].shape == self.network_weights[-1].shape)\n",
    "        self.gradient_bias[-1] = delC_delB#.reshape(self.network_bias[-1].shape)\n",
    "        assert ( self.gradient_bias[-1].shape == self.network_bias[-1].shape)\n",
    "\n",
    "        for i in reversed(range(2,self.num_of_layers)):\n",
    "            delC_delA = np.dot(self.network_weights[i-1].transpose(),delC_delZ)\n",
    "            delC_delZ = delC_delA*self.activation_function_derivative(self.Z[i-2])\n",
    "            delC_delB = np.mean(delC_delZ, axis=1)\n",
    "            delC_delW = (1.0/X.shape[1])*np.dot(delC_delZ,self.activations[i-2].transpose())\n",
    "\n",
    "            self.gradient_weights[i-2] = delC_delW\n",
    "            assert ( self.gradient_weights[i-2].shape == self.network_weights[i-2].shape)\n",
    "            self.gradient_bias[i-2] = delC_delB.reshape(self.network_bias[i-2].shape)\n",
    "            assert ( self.gradient_bias[i-2].shape == self.network_bias[i-2].shape)\n",
    "\n",
    "    def vector_to_list(self,array):\n",
    "        \n",
    "        weight_shapes = [arr_w.shape for arr_w in self.network_weights]\n",
    "        bias_shapes = [arr_b.shape for arr_b in self.network_bias]\n",
    "        \n",
    "        t=0\n",
    "        w,b = [],[]\n",
    "        for m,n in zip(weight_shapes,bias_shapes):\n",
    "            \n",
    "            ini = t\n",
    "            t = t+(m[0]*m[1])\n",
    "            assert(t-ini == m[0]*m[1])\n",
    "            w.append(array[ini : t,0].reshape(m))\n",
    "            \n",
    "            ini = t\n",
    "            t = t+(n[0]*n[1])\n",
    "            assert(t-ini == n[0]*n[1])\n",
    "            b.append(array[ini : t,0].reshape(n))\n",
    "            \n",
    "        return w,b\n",
    "        \n",
    "              \n",
    "        \n",
    "    def gradient_checking(self,X,error_method='MSE'):\n",
    "        \"\"\"Implementation of gradient checking\"\"\"\n",
    "        \n",
    "        eps = 1e-7\n",
    "        \n",
    "        conglomerate_network_array = np.array([]).reshape(-1,1)\n",
    "        conglomerate_gradient_array = np.array([]).reshape(-1,1)\n",
    "        \n",
    "        for p,r in zip(self.network_weights, self.network_bias):\n",
    "            conglomerate_network_array = np.concatenate((conglomerate_network_array,p.reshape(-1,1),r.reshape(-1,1)))\n",
    "            \n",
    "        \n",
    "        for dw,db in zip(self.gradient_weights,self.gradient_bias):\n",
    "            conglomerate_gradient_array = np.concatenate((conglomerate_gradient_array,dw.reshape(-1,1),db.reshape(-1,1)))\n",
    "        \n",
    "        #central difference\n",
    "        \n",
    "        func_plus = np.zeros_like(conglomerate_network_array)\n",
    "        func_minus = np.zeros_like(conglomerate_network_array)\n",
    "        \n",
    "        approx_graident = np.zeros_like(conglomerate_gradient_array)\n",
    "        \n",
    "        for i in range(func_plus.shape[0]):\n",
    "            \n",
    "            x_plus_eps = np.copy(conglomerate_network_array)\n",
    "            \n",
    "            x_plus_eps[i,0] = x_plus_eps[i,0]+eps\n",
    "            \n",
    "            weights_plus, bias_plus = self.vector_to_list(x_plus_eps)\n",
    "            \n",
    "            self.forward_propagation(X,check_w = weights_plus,check_b = bias_plus)\n",
    "            \n",
    "            self.cost_function(error_method)\n",
    "            \n",
    "            func_plus[i,0] = self.cost_value\n",
    "            \n",
    "            #########################################################################################\n",
    "            x_minus_eps = np.copy(conglomerate_network_array)\n",
    "            \n",
    "            x_minus_eps[i,0] = x_plus_eps[i,0]-eps\n",
    "            \n",
    "            weights_minus, bias_minus = self.vector_to_list(x_minus_eps)\n",
    "            \n",
    "            self.forward_propagation(X, check_w=weights_minus, check_b=bias_minus)\n",
    "            \n",
    "            self.cost_function(error_method)\n",
    "            \n",
    "            func_minus[i,0] = self.cost_value\n",
    "            \n",
    "            ###############################################################################################\n",
    "            approx_graident[i,0] = (func_plus[i,0] - func_minus[i,0])/(2*eps)\n",
    "            ###############################################################################################\n",
    "            #pint('the grad difference is ', approx_graident[i,0] - conglomerate_gradient_array[i,0])\n",
    "        assert(approx_graident.shape == conglomerate_gradient_array.shape)    \n",
    "        \n",
    "        numerator= np.linalg.norm(approx_graident - conglomerate_gradient_array)\n",
    "        denominator = np.linalg.norm(approx_graident)+np.linalg.norm(conglomerate_gradient_array)\n",
    "        \n",
    "        difference = numerator/denominator\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "    def update_parameters_GD(self,learning_rate):\n",
    "            ''' Implementation of gradient descent method '''\n",
    "            for p in range(1,self.num_of_layers):\n",
    "\n",
    "                self.network_weights[p-1] -= learning_rate*self.gradient_weights[p-1]\n",
    "\n",
    "                self.network_bias[p-1] -= learning_rate*self.gradient_bias[p-1]\n",
    "\n",
    "    \n",
    "    def update_parameters_Momentum(self,learning_rate,beta1):\n",
    "        '''Implementation of Gradient descent with Momentum Method'''\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.momentum_weights[p-1] = beta1*self.momentum_weights[p-1] + (1-beta1)*self.gradient_weights[p-1]\n",
    "            self.momentum_bias[p-1] = beta1*self.momentum_bias[p-1] + (1-beta1)*self.gradient_bias[p-1]\n",
    "            \n",
    "            self.network_weights[p-1] -= learning_rate*self.momentum_weights[p-1]\n",
    "            self.network_bias[p-1] -= learning_rate*self.momentum_bias[p-1]\n",
    "            \n",
    "    def update_parameters_RMSProp(self, learning_rate,beta2,epsilon=1e-8):\n",
    "        ''' Implementation of RMS Prop optimization'''\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.rms_weights[p-1]= beta2*self.rms_weights[p-1] + (1-beta2)*(self.gradient_weights[p-1])**2\n",
    "            self.rms_bias[p-1] = beta2*self.rms_bias[p-1] + (1-beta2)*(self.gradient_bias[p-1])**2\n",
    "            \n",
    "            self.network_weights[p-1] -= (learning_rate*self.gradient_weights[p-1])/(np.sqrt(self.rms_weights[p-1])+epsilon)\n",
    "            self.network_bias[p-1] -= (learning_rate*self.gradient_bias[p-1])/(np.sqrt(self.rms_bias[p-1])+epsilon)\n",
    "        \n",
    "    def update_parameters_Adam(self,learning_rate,beta1, beta2, epsilon=1e-8):\n",
    "        '''Implementation of Adaptive Moment Estimation(Adam) optimization'''\n",
    "        \n",
    "        temp_momentum_weights, temp_momentum_bias, temp_rms_weights, temp_rms_bias = [],[],[],[]\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.momentum_weights[p-1] = beta1*self.momentum_weights[p-1] + (1-beta1)*self.gradient_weights[p-1]\n",
    "            self.momentum_bias[p-1] = beta1*self.momentum_bias[p-1] + (1-beta1)*self.gradient_bias[p-1]\n",
    "            \n",
    "            temp_momentum_weights.append(self.momentum_weights[p-1]/(1-beta1**p))\n",
    "            temp_momentum_bias.append(self.momentum_bias[p-1]/(1-beta1**(p)))\n",
    "            \n",
    "            self.rms_weights[p-1]= beta2*self.rms_weights[p-1] + (1-beta2)*(self.gradient_weights[p-1])**2\n",
    "            self.rms_bias[p-1] = beta2*self.rms_bias[p-1] + (1-beta2)*(self.gradient_bias[p-1])**2\n",
    "            \n",
    "            temp_rms_weights.append(self.rms_weights[p-1]/(1-beta2**p))\n",
    "            temp_rms_bias.append(self.rms_bias[p-1]/(1-beta2**p))\n",
    "            \n",
    "            self.network_weights[p-1] -= (learning_rate*temp_momentum_weights[p-1])/(np.sqrt(temp_rms_weights[p-1])+epsilon)\n",
    "            self.network_bias[p-1] -= (learning_rate*temp_momentum_bias[p-1])/(np.sqrt(temp_rms_bias[p-1])+epsilon)\n",
    "        \n",
    "    def NN_model(self, epochs, learning_rate, beta1=None, beta2=None, activation_function = \"sigmoid\", batching=False, batch_size = None,  error_method = 'MSE', optimizer='GD'):\n",
    "        \n",
    "        ''' Deep neural network model'''\n",
    "        \n",
    "        self.network_parameters_initialization()\n",
    "        self.activation_function_name = activation_function\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        for iteration in range(epochs):\n",
    "            \n",
    "            #Batching\n",
    "            self.batching(batching=False, batch_size=None)\n",
    "            \n",
    "            #Traversing through Mini Batches:\n",
    "            for mini_batch in self.mini_batches:\n",
    "                \n",
    "                mini_batch_X, mini_batch_Y = mini_batch\n",
    "                \n",
    "                #Forward Prop\n",
    "                self.forward_propagation(mini_batch_X)\n",
    "                \n",
    "                #Loss calculation\n",
    "                self.cost_function(error_method)\n",
    "                self.cost_function_derivative()\n",
    "\n",
    "                if iteration%(epochs/10) == 0:\n",
    "                    self.cost.append(np.squeeze(self.cost_value))#np.mean(self.loss)))\n",
    "                    print('The cost after iteration: ', iteration, 'is :', self.cost_value)#np.squeeze(np.mean(self.loss)))\n",
    "                #Back Prop\n",
    "                self.back_propagation(mini_batch_X,mini_batch_Y)\n",
    "                \n",
    "                if iteration == 50:\n",
    "                    self.gradient_checking(mini_batch_X,error_method)\n",
    "                \n",
    "                if self.optimizer == 'GD':\n",
    "                 #Updating parameters with Gradient Descent\n",
    "                     self.update_parameters_GD(learning_rate)\n",
    "             \n",
    "                elif self.optimizer == 'Momentum':\n",
    "                #Updating parameters with Steepest Gradient\n",
    "                     self.update_parameters_Momentum(learning_rate, beta1)\n",
    "               \n",
    "                elif self.optimizer == 'RMSProp':\n",
    "                     self.update_parameters_RMSProp(learning_rate, beta2)\n",
    "            \n",
    "                elif self.optimizer == 'adam':\n",
    "              #Updating parameters with Adam optimization\n",
    "                     self.update_parameters_Adam(learning_rate, beta1, beta2)\n",
    "\n",
    "                    \n",
    "                \n",
    "    \n",
    "                \n",
    "        #prediction\n",
    "        prediction_train = self.training_data[0]\n",
    "        self.forward_propagation(prediction_train)\n",
    "        #\n",
    "        #plotting\n",
    "        self.plotting()\n",
    "        ##Evaluation\n",
    "        self.evaluate()\n",
    "        \n",
    "        \n",
    "    def evaluate(self):\n",
    "        prediction_test = self.testing_data[0]\n",
    "        self.forward_propagation(prediction_test)\n",
    "        \n",
    "        self.cost_function(self.error_method, test=True)\n",
    "        print('The cost in Testing is: ', self.cost_value)\n",
    "        \n",
    "    def plotting(self, test=False):\n",
    "        fig, ax = plt.subplots(nrows=3, figsize=(10,8))\n",
    "        \n",
    "        ax[0].plot(np.array(model.cost[1:]))\n",
    "        ax[0].set_title('Cost Function', fontsize = 15)\n",
    "        ax[0].set_xlabel('Iterations')\n",
    "        ax[0].set_ylabel('Cost Value')\n",
    "       # ax[0].showfig()\n",
    "        if test:\n",
    "            X ,Y = model.testing_data[0],model.testing_data[1]\n",
    "        else:\n",
    "            X,Y = model.training_data[0], model.training_data[1]\n",
    "            \n",
    "        ax[1].plot(Y, 'r--', label='Truth')\n",
    "        ax[1].plot(model.activations[-1].reshape(Y.shape), label='Predicted')\n",
    "        ax[1].set_title('Output convergence', fontsize=15)\n",
    "        ax[1].set_xlabel('Data Points')\n",
    "        ax[1].set_ylabel('Output Value i.e, Yield')\n",
    "        ax[1].legend()\n",
    "        #ax[0].showfig()\n",
    "        ax[2] = plt.axes(projection='3d')\n",
    "        ax[2].plot3D(X[0,:],X[1,:],Y, 'r--', label='Truth')\n",
    "        ax[2].plot3D(X[0,:],X[1,:],model.activations[-1].reshape(Y.shape), label='Predicted')\n",
    "        ax[2].set_title('The Yield surface', fontsize=15)\n",
    "        ax[2].legend()\n",
    "        #ax[0].showfig()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import numpy as np\n",
    "data = np.loadtxt('log.txt')\n",
    "data1 = np.zeros((data.shape[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def NN_model(self, epochs, learning_rate, beta1 , batching=False, batch_size = None,  error_method = 'MSE', optimizer='GD')\n",
    "hidden_layer_dims = [15,15]\n",
    "model = my_neural_network(hidden_layer_dims)\n",
    "file_name = 'log.txt'\n",
    "model.load_dataset(file_name)\n",
    "model.test_train_split()\n",
    "model.NN_model(40, 0.009,beta1=0.6,beta2=0.62, batching = True, batch_size= 64,optimizer = 'RMSProp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(model.cost[1:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting testing output v/s predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(model.testing_data[1],model.activations[-1].reshape(model.testing_data[1].shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.testing_data[1], 'r--')\n",
    "plt.plot(model.activations[-1].reshape(model.testing_data[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "X = model.testing_data[0]\n",
    "X1 = X[0,:]\n",
    "X2 = X[1,:]\n",
    "fig1 = plt.figure(figsize=(10,8))\n",
    "ax1 = plt.axes(projection='3d')\n",
    "ax1.plot3D(X1,X2,model.testing_data[1], 'r--')\n",
    "Y = model.activations[-1].reshape(model.testing_data[1].shape)\n",
    "ax1.plot3D(X1,X2,Y)#, 'r--')\n",
    "#X1.shape, X2.shape,model.testing_data[1].shape\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
