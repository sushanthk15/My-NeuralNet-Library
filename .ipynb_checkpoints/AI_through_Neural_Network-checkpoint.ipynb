{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_neural_network:\n",
    "    \n",
    "    def __init__(self, hidden_layer_plan):\n",
    "        \n",
    "        '''\n",
    "        Initialization function.\n",
    "        \n",
    "        input: \n",
    "        layer_plan : data_type = list\n",
    "        \n",
    "        List cointaing the number of neurons in every layers for the neural network. The first and last elements of\n",
    "        layer_plan corresponds to number of input features and the output value. \n",
    "        The rest of the elements between them corresponds to the neurons in hidden layers.\n",
    "        More the Hidden Layers implies deeper the network!\n",
    "        '''\n",
    "        \n",
    "        #dataset\n",
    "        self.dataset = None\n",
    "        \n",
    "        #input and output data\n",
    "        self.input_data = None\n",
    "        self.output_data = None\n",
    "        \n",
    "        #train and test data\n",
    "        self.training_data = None\n",
    "        self.testing_data = None\n",
    "        \n",
    "        #layer details\n",
    "        self.layer_dimensions = layer_plan\n",
    "        self.num_of_layers = len(self.layer_dimensions)\n",
    "        \n",
    "        #Batching\n",
    "        self.mini_batches = []\n",
    "        \n",
    "        #Forward Activation\n",
    "        self.activations, self.Z = [], []\n",
    "        \n",
    "        #Backward Activation\n",
    "        self.dActivations = []\n",
    "        \n",
    "        #Error_method and loss\n",
    "        self.error_method = None\n",
    "        self.loss = None\n",
    "        \n",
    "        #Network parameters\n",
    "        self.network_parameters, self.gradient_network_parameters = {}, {}\n",
    "        for i in range(1,self.num_of_layers):\n",
    "            self.network_parameters['Weights'+str(i)] = np.random.randn(self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1])\n",
    "            self.network_parameters['bias'+str(i)] = np.random.randn(self.layer_dimensions[i],1)\n",
    "            \n",
    "            assert(self.network_parameters['Weights'+str(i)].shape == (self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1]))\n",
    "            assert(self.network_parameters['bias'+str(i)].shape == (self.layer_dimensions[i],1))\n",
    "            \n",
    "            self.gradient_network_parameters['dWeights'+str(i)] = np.zeros((self.layer_dimensions[i], \n",
    "                                                                         self.layer_dimensions[i-1]))\n",
    "            self.gradient_network_parameters['dbias'+str(i)] = np.zeros((self.layer_dimensions[i],1))\n",
    "            \n",
    "        \n",
    "    def __repr__(self):\n",
    "        ''' Representative function to welcome the user for collaborating with the new AI trainer'''\n",
    "        return 'Hello there!! I am your new AI trainer......!'\n",
    "    \n",
    "    \n",
    "    def load_dataset(self, file):\n",
    "        ''' \n",
    "        To load dataset \n",
    "        \n",
    "        input:\n",
    "        file : data_type: string\n",
    "        File name consisting the inputs and outputs. The last column in the file is the output.\n",
    "        '''\n",
    "        self.dataset = np.loadtxt(file)\n",
    "        self.input_data = self.dataset[:,:-1]\n",
    "        self.output_data = self.dataset[:,-1]\n",
    "        \n",
    "        \n",
    "    def test_train_split(self, split = 0.7):\n",
    "        '''\n",
    "        This function is utilized to segragate the complete data into training and testing data.\n",
    "        '''\n",
    "        \n",
    "        n_total = int(self.dataset.shape[0])\n",
    "        n_train = int(split*n_total)\n",
    "        \n",
    "        mask = np.zeros((n_total), dtype=bool)\n",
    "        mask[:n_train] = True\n",
    "        \n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        X_train = self.input_data[mask]\n",
    "        Y_train = self.output_data[mask]\n",
    "        \n",
    "        X_test = self.input_data[~mask]\n",
    "        Y_test = self.output_data[~mask]\n",
    "        \n",
    "        self.training_data = (X_train.transpose(), Y_train)\n",
    "        self.testing_data = (X_test.transpose(), Y_test)\n",
    "        \n",
    "    def sigmoid(self,Z):\n",
    "        '''\n",
    "        A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve\n",
    "        or sigmoid curve. [Wikipedia]\n",
    "        \n",
    "        input:\n",
    "        Z = numpy_array of layer data\n",
    "        \n",
    "        returns:\n",
    "        output of sigmoid(Z)\n",
    "        '''\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    def sigmoid_der(self,Z):\n",
    "        '''\n",
    "        Function calculates the derivative of the sigmoid function.\n",
    "        \n",
    "        input:\n",
    "        Z = numpy array of layer data\n",
    "        \n",
    "        output:\n",
    "        computed derivative of the sigmoid function\n",
    "        '''\n",
    "        return sigmoid(Z)*(1-sigmoid(Z))\n",
    "    \n",
    "    def batching(self, batching = False, batch_size = None):\n",
    "        '''\n",
    "        The NN_gradient_descent function helps to build and get the deep network working. \n",
    "        \n",
    "        inputs:\n",
    "        learning_rate : floating number\n",
    "        This is an hyper parameter which is used in gradient descent\n",
    "        '''\n",
    "        num_examples = self.training_data[0].shape[1]\n",
    "              \n",
    "        \n",
    "        if batching:\n",
    "            \n",
    "            training_input = self.training_data[0]\n",
    "            training_output = self.training_data[1]\n",
    "            \n",
    "            #mini_batches = []\n",
    "            \n",
    "            number_of_batches = int(num_examples/batch_size)\n",
    "            \n",
    "            for j in range(0,number_of_batches):\n",
    "                mini_train_input = training_input[:,(j*batch_size):((j+1)*batch_size)]\n",
    "                mini_train_output = training_output[:,(j*batch_size):((j+1)*batch_size)]\n",
    "                self.mini_batches.append((mini_train_input,mini_train_output))\n",
    "                \n",
    "            if num_examples % batch_size != 0:\n",
    "                mini_train_input = training_input[:,(number_of_batches*batch_size):]\n",
    "                mini_train_output = training_output[:,(number_of_batches*batch_size):]\n",
    "                self.mini_batches.append((mini_train_input,mini_train_output))\n",
    "        else:\n",
    "            \n",
    "            self.mini_batches = [self.training_data]\n",
    "            \n",
    "    def forward_propagation(self,X):\n",
    "        '''\n",
    "        Implementation of forward propagation i.e, activating the linear function\n",
    "        \n",
    "        Process: Z = WX +b\n",
    "                Activation = Sigmoid(Z)\n",
    "        '''\n",
    "        \n",
    "        self.activations = [X]\n",
    "        \n",
    "        for l in range(1,self.num_of_layers):\n",
    "            Z = np.dot(self.network_parameters['Weights'+str(l)], self.activations[l-1]) + self.network_parameters['bias'\n",
    "                                                                                              +str(l)]\n",
    "            assert(Z.shape == (self.network_parameters['Weights'+str(l)].shape[0], self.activations[l-1].shape[1]))\n",
    "            \n",
    "                        \n",
    "            activated_Z = self.sigmoid(Z)\n",
    "            \n",
    "            assert(activated_Z.shape == Z.shape)\n",
    "            \n",
    "            self.Z.append(Z)\n",
    "            self.activations.append(activated_Z)\n",
    "    \n",
    "    def loss_function(self, error_method = 'MSE'):\n",
    "        \n",
    "        '''Calculates the loss/error between the predicted values and the actual output'''\n",
    "        self.error_method = error_method\n",
    "        if error_method == 'MSE':\n",
    "            self.loss = self.activations[-1] - self.training_data[1]\n",
    "            \n",
    "                    \n",
    "    def back_propagation(self, X, Y):\n",
    "        ''' Implementation of backward propagation \n",
    "        Process:\n",
    "        1. dLoss w.r.t dActivation of Last layer\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        delta = self.loss\n",
    "        self.gradient_network_parameters['dbias'+str(self.num_of_layers)] = delta\n",
    "        self.gradient_network_parameters['dWeights'+str(self.num_of_layers)] = np.dot(delta, \n",
    "                                                                                      self.activations[-2].transpose())\n",
    "        \n",
    "        for m in reversed(range(self.num_of_layers)):\n",
    "            \n",
    "            sigmoid_derivative = self.sigmoid.der(self.Z[m])\n",
    "            \n",
    "            delta = np.dot(self.network_parameters['Weights'+str(m+1)].transpose(), delta) * sigmoid_derivative\n",
    "            \n",
    "            self.gradient_network_parameters['dbias'+str(m+1)] = delta\n",
    "            \n",
    "            self.gradient_network_parameters['dWeights'+str(m+1)] = np.dot(delta, self.activations[m-1].transpose())\n",
    "            \n",
    "    def update_parameters_GD(self,learning_rate):\n",
    "        ''' Implementation of gradient descent method '''\n",
    "        \n",
    "        for p in range(1,self.num_of_layers):\n",
    "            \n",
    "            self.network_parameters['Weights'+str(p)] -= learning_rate*self.gradient_network_parameters['dWeights'\n",
    "                                                                                                        +str(p)] \n",
    "            self.network_parameters['bias'+str(p)] -= learning_rate*self.gradient_network_parameters['dbias'\n",
    "                                                                                                    +str(p)]\n",
    "            \n",
    "              \n",
    "    def NN_model(self, batching=False, batch_size = None, epochs, error_method = 'MSE',learning_rate):\n",
    "        \n",
    "        ''' Deep neural network model'''\n",
    "        \n",
    "        for iteration in range(epochs):\n",
    "            \n",
    "            #Batching\n",
    "            self.batching(batching=False, batch_size=None)\n",
    "            \n",
    "            #Traversing through Mini Batches:\n",
    "            for mini_batch in self.mini_batches:\n",
    "                \n",
    "                mini_batch_X, mini_batch_Y = mini_batch\n",
    "                \n",
    "                #Forward Prop\n",
    "                self.forward_propagation(mini_batch_X)\n",
    "                \n",
    "                #Loss calculation\n",
    "                self.loss_function(error_method)\n",
    "                \n",
    "                #Back Prop\n",
    "                self.back_propagation(mini_batch_X, mini_batch_Y)\n",
    "                \n",
    "                #Updating parameters with Gradient Descent\n",
    "                self.update_parameters_GD(learning_rate)\n",
    "                \n",
    "        #prediction\n",
    "        prediction_train = self.training_data[0]\n",
    "        self.forward_propagation(prediction_train)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [2,5,3,1]\n",
    "model = my_neural_network(layer_dims)\n",
    "file_name = 'log.txt'\n",
    "model.load_dataset(file_name)\n",
    "model.test_train_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 700)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 5\n",
    "p-= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
